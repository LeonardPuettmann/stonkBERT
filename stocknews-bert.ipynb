{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"stock-news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.headline.values\n",
    "labels_string = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for label in labels_string:\n",
    "    if label == \"Positive\":\n",
    "        labels.append(0)\n",
    "    elif label == \"Neutral\":\n",
    "        labels.append(1)\n",
    "    elif label == \"Negative\":\n",
    "        labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤═════════════╕\n",
      "│ Tokens    │   Token IDs │\n",
      "╞═══════════╪═════════════╡\n",
      "│ net       │        5795 │\n",
      "├───────────┼─────────────┤\n",
      "│ ##f       │        2087 │\n",
      "├───────────┼─────────────┤\n",
      "│ ##lix     │       20711 │\n",
      "├───────────┼─────────────┤\n",
      "│ lays      │       22307 │\n",
      "├───────────┼─────────────┤\n",
      "│ offs      │       12822 │\n",
      "├───────────┼─────────────┤\n",
      "│ more      │        1167 │\n",
      "├───────────┼─────────────┤\n",
      "│ employees │        4570 │\n",
      "├───────────┼─────────────┤\n",
      "│ to        │        1106 │\n",
      "├───────────┼─────────────┤\n",
      "│ cut       │        2195 │\n",
      "├───────────┼─────────────┤\n",
      "│ costs     │        4692 │\n",
      "╘═══════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence():\n",
    "  '''Displays the tokens and respective IDs of a random text sample'''\n",
    "  index = random.randint(0, len(text)-1)\n",
    "  table = np.array([tokenizer.tokenize(text[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "  print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\leopu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 32,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                    )\n",
    "\n",
    "\n",
    "for sample in text:\n",
    "  encoding_dict = preprocessing(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,   190,   119,   188,   119,   175,  1810, 11080,   171,  7897,\n",
       "         2083,   112,   188, 19192,  4442,  9148,  1112,  1211,  3021,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]    │         101 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ will     │        1209 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ health   │        2332 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ revenue  │        7143 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ growth   │        3213 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ b        │         171 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##uo     │       11848 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##y      │        1183 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ j        │         179 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##abi    │       23156 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##l      │        1233 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ (        │         113 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ j        │         179 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##b      │        1830 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##l      │        1233 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ )        │         114 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ q        │         186 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##3      │        1495 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ earnings │       18155 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ?        │         136 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]    │         102 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text) - 1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  \n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n",
    "# Run on GPU\n",
    "model.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
